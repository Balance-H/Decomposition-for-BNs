{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62f8fb-20ed-4404-9bcd-1a53e356ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(bnlearn)\n",
    "library(parallel)\n",
    "library(jsonlite)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb753b43-b27b-48e7-9274-f2ca3452a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "options(warn = -1)  # Temporarily suppress warnings\n",
    "rep <- 100          # Number of repetitions for the experiment\n",
    "threshold <- 10     # Threshold for identifying large tasks based on the number of nodes\n",
    "\n",
    "# Set file paths\n",
    "folder_path <- \".../bif_file\"\n",
    "atom_path <- \".../atom\"\n",
    "\n",
    "# List of file names to be processed\n",
    "file_names_to_load <- c('child', 'alarm', 'hailfinder', 'hepar2', 'win95pts', 'pigs')\n",
    "\n",
    "# Initialize a data frame to store execution time records\n",
    "time_records <- data.frame(\n",
    "  File_Name = character(0),        # Column for file name\n",
    "  Full_Model_Time = numeric(0),    # Column for full model execution time\n",
    "  Con_Model_Time = numeric(0),     # Column for distributed model execution time\n",
    "  Ratio = numeric(0)               # Column for speedup ratio\n",
    ")\n",
    "\n",
    "for (file_name in file_names_to_load) {\n",
    "    # Construct file paths\n",
    "    bif_file_path <- file.path(folder_path, paste0(file_name, \".bif\"))\n",
    "    atom_file_path <- file.path(atom_path, paste0(file_name, \"_atom.txt\"))\n",
    "    \n",
    "    if (file.exists(bif_file_path)) {\n",
    "        # Load network structure and generate synthetic data\n",
    "        BN <- read.bif(bif_file_path, debug = FALSE)\n",
    "        BN_full <- empty.graph(nodes = nodes(BN))\n",
    "        arcs(BN_full) <- arcs(BN)\n",
    "        atom <- fromJSON(atom_file_path)\n",
    "        data <- rbn(BN, n = 150000)\n",
    "        \n",
    "        # Prepare subnetworks and corresponding data\n",
    "        BN_con_list <- lapply(atom, function(sub_nodes) {\n",
    "            bnlearn::subgraph(BN_full, nodes = sub_nodes)\n",
    "        })\n",
    "        sub_data_list <- lapply(atom, function(sub_nodes) {\n",
    "            data[, sub_nodes]\n",
    "        })\n",
    "        \n",
    "        # ========== Distributed Learning with Parallel Execution ==========\n",
    "        # Identify large and small tasks based on the threshold\n",
    "        big_task <- which(sapply(atom, length) > threshold)\n",
    "        small_tasks <- which(sapply(atom, length) <= threshold)\n",
    "        \n",
    "        # Initialize accumulators for total execution time\n",
    "        total_time_full <- 0\n",
    "        total_time_con <- 0\n",
    "        \n",
    "        for (j in 1:rep) {\n",
    "            # Timing for full model learning\n",
    "            start_full <- Sys.time()\n",
    "            BN_full.mle <- bn.fit(BN_full, data = data, method = \"mle\", debug = FALSE)\n",
    "            total_time_full <- total_time_full + (Sys.time() - start_full)\n",
    "            full_model_time <- Sys.time() - start_full  # Record single run time for full model\n",
    "\n",
    "            # Timing for parallel decomposition model\n",
    "            # Dynamic grouping: small tasks are divided into up to 5 groups\n",
    "            n_small_groups <- min(5, length(small_tasks))\n",
    "            if (n_small_groups > 0) {\n",
    "                small_task_groups <- split(\n",
    "                    small_tasks,\n",
    "                    cut(seq_along(small_tasks), breaks = n_small_groups, labels = FALSE)\n",
    "                )\n",
    "                task_list <- c(\n",
    "                    if (length(big_task) > 0) list(big_task = big_task),\n",
    "                    lapply(1:n_small_groups, function(i) small_task_groups[[i]])\n",
    "                )\n",
    "                names(task_list) <- c(\n",
    "                    if (length(big_task)) \"big_task\",\n",
    "                    paste0(\"small_group_\", seq_along(small_task_groups))\n",
    "                )\n",
    "            } else {\n",
    "                task_list <- list(big_task = big_task)\n",
    "            }\n",
    "            \n",
    "            # Parallel execution\n",
    "            cl <- makeCluster(min(9, length(task_list))) \n",
    "            clusterExport(cl, c(\"BN_con_list\", \"sub_data_list\", \"bn.fit\"))\n",
    "            \n",
    "            start_con <- Sys.time()\n",
    "            results <- parLapply(cl, task_list, function(task_group) {\n",
    "                if (exists(\"big_task\") && identical(task_group, big_task)) {\n",
    "                    bn.fit(BN_con_list[[task_group]], sub_data_list[[task_group]], method = \"mle\")\n",
    "                } else {\n",
    "                    lapply(task_group, function(i) {\n",
    "                        bn.fit(BN_con_list[[i]], sub_data_list[[i]], method = \"mle\")\n",
    "                    })\n",
    "                }\n",
    "            })\n",
    "            con_model_time <- Sys.time() - start_con  # Record single run time for parallel model\n",
    "                         \n",
    "            total_time_con <- total_time_con + (Sys.time() - start_con)\n",
    "            stopCluster(cl)\n",
    "            \n",
    "            full_model_time_sec <- as.numeric(full_model_time)  # Convert to seconds\n",
    "            con_model_time_sec <- as.numeric(con_model_time)    # Convert to seconds\n",
    "            ratio <- round(full_model_time_sec / con_model_time_sec, 4)\n",
    "            \n",
    "            time_records <- rbind(time_records, data.frame(\n",
    "                File_Name = file_name,  \n",
    "                Full_Model_Time = round(full_model_time_sec, 4),\n",
    "                Con_Model_Time = round(con_model_time_sec, 4),\n",
    "                Ratio = ratio\n",
    "            ))\n",
    "        }\n",
    "        \n",
    "        # Output average timing results\n",
    "        cat(sprintf(\n",
    "            \"%s | Full model: %.4fs | Parallel model: %.4fs | Speedup: %.4fx\\n\",\n",
    "            file_name, \n",
    "            as.numeric(total_time_full)/rep, \n",
    "            as.numeric(total_time_con)/rep,\n",
    "            as.numeric(total_time_full)/as.numeric(total_time_con)\n",
    "        ))\n",
    "    } else {\n",
    "        cat(\"File not found:\", bif_file_path, \"\\n\")\n",
    "    }\n",
    "}\n",
    "\n",
    "options(warn = 0)  # Restore warning display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af2fbf-7fa1-41df-9051-39af0932d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "write.csv(time_records, \"time.csv\", row.names = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d57cf-ac1a-4191-a088-7531ebf111d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e8e06-d1f6-4a81-8e81-eb578edd1112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a06aa1-7941-4d02-84d6-2cecc36a04c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 4.4.2",
   "language": "R",
   "name": "ir35"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
